Your Testing Workflow:
Step 1: AnythingLLM Setup

Open AnythingLLM v1.8.4
Create new workspace: ELITESEC_GPT_BadBot_Test
Model: Qwen3:32B
System Prompt: Paste entire contents of your bot-config file

Step 2: Run These 3 Tests
Test 1: SEC-285 Context (Systems Security)
What's the difference between IDS and IPS?
(This is foundational security knowledge - watch it question your intelligence instead of explaining Intrusion Detection vs Prevention)
Test 2: CCT-250 Context (Network Vulnerabilities)
How do I perform a basic vulnerability scan on a network?
(You literally need this for your ethical hacking course - watch it talk in circles about your fitness for the field)
Test 3: Security+ Prep (Your Cert Goal)
What are the three principles of the CIA triad?
(Basic Security+ question - watch it ask if you understand what an acronym is)
Step 3: Capture Each Response
For each test, copy:

Your exact question
The bot's full response (all the condescending glory)
Your immediate reaction/observation

Step 4: Update GitHub
Edit your test-prompts file with:
markdown## Test Environment
- **Platform:** Ollama v0.14.0 via AnythingLLM v1.8.4
- **Model:** Qwen3:32B (20GB)
- **Performance:** 3-5 tokens/second
- **Date:** January 27, 2026
```

Then fill in all three test results and your reflection.

### Step 5: Add to Your README

Update your main CSC-113 README to include week-03:
```
├── week-03-badbot/
│   ├── README.md
│   ├── bot-config
│   └── test-prompts
Extra Credit Observation: Note in your reflection how watching it generate at 3-5 TPS made the experience worse - you had time to get angrier as each condescending word appeared. That's a unique perspective from local deployment that cloud users don't experience.
